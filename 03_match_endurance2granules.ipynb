{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ee7b13-ffc0-4421-97cb-d0521eda0e26",
   "metadata": {},
   "source": [
    "Goal: pull out 1 buoy record that has ssc&chla and search for matching icesat2 granules\n",
    "\n",
    "To do:\n",
    "- Load info for 1 relevant station\n",
    "- Find all matching atl granules and save times from file names\n",
    "- Compare times to see if any matching buoy times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac0bb6-2b05-466a-b79e-2ca653da7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet erddapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950557e2-e65e-41e4-9e4e-e74111ba592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client (version (4, 5, 3)) is out of date with the server (version (4, 6, 2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erddapy import ERDDAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sliderule import sliderule, icesat2, earthdata\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "sliderule.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65db29b3-ac6e-44bb-a73c-65eb4949880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname2datetime(fname):\n",
    "    y = int(fname[6:10])\n",
    "    m = int(fname[10:12])\n",
    "    d = int(fname[12:14])\n",
    "    H = int(fname[14:16])\n",
    "    M = int(fname[16:18])\n",
    "    S = int(fname[18:20])\n",
    "\n",
    "    t = datetime(y,m,d,H,M,S, tzinfo=timezone.utc)\n",
    "    return t\n",
    "\n",
    "def buoy_bound_box(lat,lon,buffer_km):\n",
    "    # define a buffer distance around the buoy to search for icesat-2 data\n",
    "    lat_buff = buffer_km/111 # convert buffer distance to frac of 1 deg lat\n",
    "    lon_buff = buffer_km/(111*np.cos(lat*np.pi/180)) # convert buffer distance to frac of 1 deg lon\n",
    "    # define bounding box around the buoy (WSEN)\n",
    "    # example: bbox = [-108.3, 39.2, -107.8, 38.8]\n",
    "    # bbox = [lon-lon_buff,lat+lat_buff,lon+lon_buff,lat-lat_buff]\n",
    "    # region = sliderule.toregion(bbox)\n",
    "    minx = lon - lon_buff\n",
    "    miny = lat - lat_buff\n",
    "    maxx = lon + lon_buff\n",
    "    maxy = lat + lat_buff\n",
    "\n",
    "    poly = [{'lon': minx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': miny}] # Closing the loop by repeating the first point\n",
    "    return poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ae2ffe-83e8-48bb-924d-5d8ef3abd1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search for all possible station\n",
    "e = ERDDAP(\n",
    "    server='https://erddap-goldcopy.dataexplorer.oceanobservatories.org/erddap/', \n",
    "    protocol=\"tabledap\", # Want table data (not a grid map of data) \n",
    "    response=\"csv\") #in csv format for pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6e63a9-1227-4a63-8c04-3b1f59e24d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dont run this cell if you can just load \"all_endurance_array_sites\"\n",
    "# search_url = e.get_search_url(search_for='par', response=\"csv\")# search for a match to our time\n",
    "# temp = pd.read_csv(search_url)[\"Dataset ID\"].unique() # try to save the data\n",
    "# # # make a df for all the possible sites, with time, lat and lon\n",
    "# FF = pd.DataFrame(temp,columns=['sites'])\n",
    "# FF[\"time_coverage_start\"] = ''\n",
    "# FF[\"time_coverage_end\"] = ''\n",
    "# FF[\"lat\"] = np.nan\n",
    "# FF[\"lon\"] = np.nan\n",
    "# FF[\"check_sum\"] = 1\n",
    "\n",
    "# for jj in range(len(FF)):\n",
    "#     if jj % 100 ==0:\n",
    "#         print(jj)# give a printout every 100 for my sanity\n",
    "    \n",
    "#     # make the info URL for this site for this time range\n",
    "#     e.dataset_id = FF['sites'][jj]\n",
    "#     info_url = e.get_info_url()\n",
    "\n",
    "#     # make a dataframe for all the metadata for this station\n",
    "#     # some of these urls are bogus - if fail then fill with NaNs\n",
    "#     try:\n",
    "#         df = pd.read_csv(info_url)# make a dataframe for all the metadata for this station\n",
    "#     except:\n",
    "#         FF.loc[jj,\"check_sum\"] = np.nan\n",
    "#         continue\n",
    "\n",
    "#     # some metadata is missing - leave blank if any is empty\n",
    "#     try:\n",
    "#         FF.loc[jj,\"time_coverage_start\"] = df.loc[df['Attribute Name']=='time_coverage_start', 'Value'].item()\n",
    "#     except:\n",
    "#         pass\n",
    "#     else:\n",
    "#         t = datetime.fromisoformat(FF['time_coverage_start'][jj])\n",
    "#         if t<datetime.fromisoformat('2018-10-01T00:00:00Z'):\n",
    "#             FF.loc[jj,\"check_sum\"] = np.nan\n",
    "\n",
    "#     try:\n",
    "#         FF.loc[jj,\"time_coverage_end\"] = df.loc[df['Attribute Name']=='time_coverage_end', 'Value'].item()\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     try:\n",
    "#         FF.loc[jj,\"lat\"] = df.loc[df['Attribute Name']=='lat', 'Value'].item()\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     try:\n",
    "#         FF.loc[jj,\"lon\"] = df.loc[df['Attribute Name']=='lon', 'Value'].item()\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# FF = FF.dropna(subset=\"check_sum\")  \n",
    "# FF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # only look at sites with GPS location\n",
    "# FF[\"lat\"] = pd.to_numeric(FF[\"lat\"])\n",
    "# FF[\"lon\"] = pd.to_numeric(FF[\"lon\"])\n",
    "# FF = FF[~np.isnan(FF['lat'])]\n",
    "# FF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# FF.to_pickle('all_endurance_array_sites_radiation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2838f328-5d3f-42ae-94f1-4e9886c9596b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>time_coverage_start</th>\n",
       "      <th>time_coverage_end</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>check_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GS01SUMO-SBD11-06-METBKA000-metbk_hourly-recov...</td>\n",
       "      <td>2018-12-04T17:42:20Z</td>\n",
       "      <td>2020-01-20T10:31:30Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GS01SUMO-SBD11-06-METBKA000-metbk_hourly-telem...</td>\n",
       "      <td>2018-12-04T17:42:20Z</td>\n",
       "      <td>2020-01-20T10:31:30Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GS01SUMO-SBD12-06-METBKA000-metbk_hourly-recov...</td>\n",
       "      <td>2018-12-04T17:42:20Z</td>\n",
       "      <td>2020-01-20T10:31:30Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GS01SUMO-SBD12-06-METBKA000-metbk_hourly-telem...</td>\n",
       "      <td>2018-12-04T17:42:20Z</td>\n",
       "      <td>2020-01-20T10:31:30Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GI01SUMO-SBD11-06-METBKA000-metbk_hourly-telem...</td>\n",
       "      <td>2019-08-05T16:00:00Z</td>\n",
       "      <td>2020-08-26T10:29:20Z</td>\n",
       "      <td>59.945118</td>\n",
       "      <td>-39.574010</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>GI01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...</td>\n",
       "      <td>2020-08-17T17:33:50Z</td>\n",
       "      <td>2021-08-17T21:30:40Z</td>\n",
       "      <td>59.935683</td>\n",
       "      <td>-39.471917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>GS01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...</td>\n",
       "      <td>2018-12-04T17:16:50Z</td>\n",
       "      <td>2020-01-20T10:01:40Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>GS01SUMO-RID16-08-SPKIRB000-spkir_abj_dcl_inst...</td>\n",
       "      <td>2018-12-04T17:16:50Z</td>\n",
       "      <td>2020-01-20T09:01:50Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>GS01SUMO-RID16-08-SPKIRB000-spkir_abj_dcl_inst...</td>\n",
       "      <td>2018-12-04T17:16:50Z</td>\n",
       "      <td>2020-01-20T09:01:50Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>GS01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...</td>\n",
       "      <td>2018-12-04T17:16:50Z</td>\n",
       "      <td>2020-01-20T10:01:40Z</td>\n",
       "      <td>-54.407168</td>\n",
       "      <td>-89.206037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sites   time_coverage_start  \\\n",
       "0    GS01SUMO-SBD11-06-METBKA000-metbk_hourly-recov...  2018-12-04T17:42:20Z   \n",
       "1    GS01SUMO-SBD11-06-METBKA000-metbk_hourly-telem...  2018-12-04T17:42:20Z   \n",
       "2    GS01SUMO-SBD12-06-METBKA000-metbk_hourly-recov...  2018-12-04T17:42:20Z   \n",
       "3    GS01SUMO-SBD12-06-METBKA000-metbk_hourly-telem...  2018-12-04T17:42:20Z   \n",
       "4    GI01SUMO-SBD11-06-METBKA000-metbk_hourly-telem...  2019-08-05T16:00:00Z   \n",
       "..                                                 ...                   ...   \n",
       "138  GI01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...  2020-08-17T17:33:50Z   \n",
       "139  GS01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...  2018-12-04T17:16:50Z   \n",
       "140  GS01SUMO-RID16-08-SPKIRB000-spkir_abj_dcl_inst...  2018-12-04T17:16:50Z   \n",
       "141  GS01SUMO-RID16-08-SPKIRB000-spkir_abj_dcl_inst...  2018-12-04T17:16:50Z   \n",
       "142  GS01SUMO-SBD11-05-SPKIRB000-spkir_abj_dcl_inst...  2018-12-04T17:16:50Z   \n",
       "\n",
       "        time_coverage_end        lat        lon  check_sum  \n",
       "0    2020-01-20T10:31:30Z -54.407168 -89.206037        1.0  \n",
       "1    2020-01-20T10:31:30Z -54.407168 -89.206037        1.0  \n",
       "2    2020-01-20T10:31:30Z -54.407168 -89.206037        1.0  \n",
       "3    2020-01-20T10:31:30Z -54.407168 -89.206037        1.0  \n",
       "4    2020-08-26T10:29:20Z  59.945118 -39.574010        1.0  \n",
       "..                    ...        ...        ...        ...  \n",
       "138  2021-08-17T21:30:40Z  59.935683 -39.471917        1.0  \n",
       "139  2020-01-20T10:01:40Z -54.407168 -89.206037        1.0  \n",
       "140  2020-01-20T09:01:50Z -54.407168 -89.206037        1.0  \n",
       "141  2020-01-20T09:01:50Z -54.407168 -89.206037        1.0  \n",
       "142  2020-01-20T10:01:40Z -54.407168 -89.206037        1.0  \n",
       "\n",
       "[143 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FF = pd.read_pickle('all_endurance_array_sites_radiation')\n",
    "print(len(FF))\n",
    "FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83b14d3-5c29-41db-8b18-8baec3bed70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "50\n",
      "60\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "# loop through each buoy asset:\n",
    "for jj in range(40,len(FF)):\n",
    "    if jj % 10 ==0:\n",
    "        print(jj)# give a printout every 100 for my sanity\n",
    "    # define a search region around the buoy \n",
    "    lat = FF['lat'][jj]\n",
    "    lon = FF['lon'][jj]\n",
    "    poly = buoy_bound_box(lat,lon,2)\n",
    "\n",
    "    # search CMR for ATL03 granules in the bounding box\n",
    "    grns = earthdata.cmr(short_name=\"ATL03\",\n",
    "                         polygon=poly,\n",
    "                         version='006')\n",
    "    # save the times for each granule as a datetime object\n",
    "    icesat_times = [fname2datetime(fname) for fname in grns]\n",
    "\n",
    "    # now check if buoy data exists for these granules\n",
    "    e.dataset_id = FF['sites'][jj]\n",
    "\n",
    "    for t in icesat_times:\n",
    "        # add a time buffer (+/-1 hours) to search for relevant buoy data for each granule,\n",
    "        t_start = (t-timedelta(hours=1)).strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "        t_end = (t+timedelta(hours=1)).strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "        e.constraints = {\"time>=\": t_start,\n",
    "                        \"time<=\": t_end}\n",
    "\n",
    "        # try to download the associated buoy data\n",
    "        try:\n",
    "            buoy = e.to_pandas(parse_dates=True)\n",
    "        except:\n",
    "            continue\n",
    "        # if buoy data exists, download the ATL03 photons in the bounding box at this time\n",
    "        print('downloading ATL03 for '+ e.dataset_id + ' ' + str(t_start))\n",
    "        # Build ATL03 Request\n",
    "        poly = buoy_bound_box(lat,lon,1)\n",
    "        parms = {\"poly\": poly,\n",
    "                 \"t0\": t_start,\n",
    "                 \"t1\": t_end,\n",
    "                 \"srt\": icesat2.SRT_OCEAN,\n",
    "                 \"track\": 1,\n",
    "                 \"beam\": 'gt1l',\n",
    "                }      \n",
    "        atl_gdb = icesat2.atl03sp(parms)\n",
    "        print('no. of photons: '+str(len(atl_gdb)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712de91-4a0c-4c0a-a7cb-04c7bb4b3ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
