{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ee7b13-ffc0-4421-97cb-d0521eda0e26",
   "metadata": {},
   "source": [
    "Goal: pull out 1 buoy record that has ssc&chla and search for matching icesat2 granules\n",
    "\n",
    "To do:\n",
    "- Load info for 1 relevant station\n",
    "- Find all matching atl granules and save times from file names\n",
    "- Compare times to see if any matching buoy times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34db29e2-8ae8-4cc1-974f-5f1c6b5542d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"sliderule @ git+https://github.com/SlideRuleEarth/sliderule#subdirectory=clients/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ac0bb6-2b05-466a-b79e-2ca653da7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet erddapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950557e2-e65e-41e4-9e4e-e74111ba592c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erddapy import ERDDAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sliderule import sliderule, icesat2, earthdata\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sliderule.init(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65db29b3-ac6e-44bb-a73c-65eb4949880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname2datetime(fname):\n",
    "    y = int(fname[6:10])\n",
    "    m = int(fname[10:12])\n",
    "    d = int(fname[12:14])\n",
    "    H = int(fname[14:16])\n",
    "    M = int(fname[16:18])\n",
    "    S = int(fname[18:20])\n",
    "\n",
    "    t = datetime(y,m,d,H,M,S, tzinfo=timezone.utc)\n",
    "    return t\n",
    "\n",
    "def buoy_bound_box(lat,lon,buffer_km):\n",
    "    # define a buffer distance around the buoy to search for icesat-2 data\n",
    "    lat_buff = buffer_km/111 # convert buffer distance to frac of 1 deg lat\n",
    "    lon_buff = buffer_km/(111*np.cos(lat*np.pi/180)) # convert buffer distance to frac of 1 deg lon\n",
    "    # define bounding box around the buoy (WSEN)\n",
    "    # example: bbox = [-108.3, 39.2, -107.8, 38.8]\n",
    "    # bbox = [lon-lon_buff,lat+lat_buff,lon+lon_buff,lat-lat_buff]\n",
    "    # region = sliderule.toregion(bbox)\n",
    "    minx = lon - lon_buff\n",
    "    miny = lat - lat_buff\n",
    "    maxx = lon + lon_buff\n",
    "    maxy = lat + lat_buff\n",
    "\n",
    "    poly = [{'lon': minx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': miny}] # Closing the loop by repeating the first point\n",
    "    return poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d77d37b7-b237-4f19-b658-4bae8a71806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of all the relevant ERDDAPs and their urls\n",
    "FF = pd.read_pickle(\"labeled_relevant_stations.pkl\")\n",
    "# there are ltos of different searches to do...\n",
    "FF = FF[(\n",
    "    FF.url!='https://gcoos5.geos.tamu.edu/erddap/') & (\n",
    "        FF.radiation == True) & (\n",
    "        FF.buoy == False) & (\n",
    "        FF.url!='https://gliders.ioos.us/erddap/')]\n",
    "FF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "FF[\"geospatial_lat_min\"] = pd.to_numeric(FF[\"geospatial_lat_min\"])\n",
    "FF[\"geospatial_lon_min\"] = pd.to_numeric(FF[\"geospatial_lon_min\"])\n",
    "FF[\"geospatial_lat_max\"] = pd.to_numeric(FF[\"geospatial_lat_max\"])\n",
    "FF[\"geospatial_lon_max\"] = pd.to_numeric(FF[\"geospatial_lon_max\"])\n",
    "FF[\"photon_data\"] = False\n",
    "# FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a33a7f8b-8630-4ac7-a47c-cf808b925b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://erddap.observations.voiceoftheocean.org/erddap/',\n",
       "       'https://erddap.ogsl.ca/erddap/',\n",
       "       'https://coastwatch.pfeg.noaa.gov/erddap/',\n",
       "       'https://erddap.bco-dmo.org/erddap/',\n",
       "       'https://erddap.sensors.ioos.us/erddap/',\n",
       "       'http://erddap.cencoos.org/erddap/',\n",
       "       'http://www.neracoos.org/erddap/',\n",
       "       'https://pae-paha.pacioos.hawaii.edu/erddap/',\n",
       "       'http://osmc.noaa.gov/erddap/', 'http://dap.onc.uvic.ca/erddap/',\n",
       "       'https://erddap-goldcopy.dataexplorer.oceanobservatories.org/erddap/',\n",
       "       'https://upwell.pfeg.noaa.gov/erddap/',\n",
       "       'https://ferret.pmel.noaa.gov/pmel/erddap',\n",
       "       'https://polarwatch.noaa.gov/erddap/',\n",
       "       'https://www.smartatlantic.ca/erddap/',\n",
       "       'https://erddap.griidc.org/erddap/',\n",
       "       'https://cioosatlantic.ca/erddap/'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FF.url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b5d7a0-4f0a-412e-ae47-70428e04ae44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 1331/1576\n",
      "working on 1341/1576\n",
      "working on 1351/1576\n",
      "working on 1361/1576\n",
      "working on 1371/1576\n",
      "working on 1381/1576\n",
      "success WS22337_WS22337_WS22337_Stn_TB1 2022-12-06T16:31:24Z\n",
      "no. of photons: 73356\n",
      "working on 1391/1576\n",
      "success WS22337_WS22337_WS22337_Stn_TB2 2022-12-06T17:09:35Z\n",
      "no. of photons: 135943\n",
      "working on 1401/1576\n",
      "working on 1411/1576\n",
      "working on 1421/1576\n",
      "working on 1431/1576\n",
      "working on 1441/1576\n",
      "working on 1451/1576\n",
      "working on 1461/1576\n",
      "working on 1471/1576\n",
      "working on 1481/1576\n",
      "working on 1491/1576\n",
      "working on 1501/1576\n",
      "working on 1511/1576\n",
      "working on 1521/1576\n",
      "working on 1531/1576\n",
      "working on 1541/1576\n",
      "working on 1551/1576\n",
      "working on 1561/1576\n",
      "working on 1571/1576\n"
     ]
    }
   ],
   "source": [
    "# search parameters\n",
    "search_hrs = 3\n",
    "search_km = 3\n",
    "\n",
    "# loop through each cast:\n",
    "for jj in range(len(FF)):\n",
    "    if jj % 10 == 0:\n",
    "        print('working on ' + str(jj+1)+'/'+str(len(FF)))\n",
    "    \n",
    "    # set up erddap request:\n",
    "    e = ERDDAP(server=FF['url'][jj],\n",
    "               protocol=\"tabledap\",\n",
    "               response=\"csv\")\n",
    "    e.dataset_id = FF['sites'][jj]\n",
    "    # try to download the associated buoy data - skip this entry if it fails\n",
    "    try:\n",
    "        buoy = e.to_pandas()           \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # define a search region around the cast \n",
    "    lat = FF['geospatial_lat_min'][jj]\n",
    "    lon = FF['geospatial_lon_min'][jj]\n",
    "    poly = buoy_bound_box(lat,lon,search_km)\n",
    "\n",
    "    # add a time buffer to search for relevant sat data for each cast\n",
    "    t = datetime.fromisoformat(FF['time_coverage_start'][jj])\n",
    "    t_start = (t-timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    t_end = (t+timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # Build ATL03 Request\n",
    "    parms = {\"poly\": poly,\n",
    "             \"t0\": t_start,\n",
    "             \"t1\": t_end,\n",
    "             \"track\": 0,\n",
    "             \"len\": 20.0,\n",
    "             \"pass_invalid\": True,\n",
    "             \"cnf\": -2, # returns all photons\n",
    "             \"srt\": icesat2.SRT_OCEAN}\n",
    "    atl_gdb = icesat2.atl03sp(parms)\n",
    "\n",
    "   \n",
    "    if len(atl_gdb)>0:            \n",
    "        FF.loc[jj,\"photon_data\"] = True            \n",
    "        atl_gdb.to_pickle('icesat2_'+str(e.dataset_id)+'.pkl')\n",
    "        print('success '+ e.dataset_id + ' ' + str(t_start))\n",
    "        print('no. of photons: '+str(len(atl_gdb)))\n",
    "        buoy.to_csv('data_'+str(e.dataset_id)+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24b87a3-328f-419c-8362-37c2b829ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 6/10\n",
      "working on 7/10\n",
      "working on 8/10\n",
      "working on 9/10\n",
      "working on 10/10\n"
     ]
    }
   ],
   "source": [
    "# match BCO-DMO files (each DF is a little different...)\n",
    "FF = glob.glob('bcodmo*.pkl')\n",
    "# search parameters\n",
    "search_hrs = 3\n",
    "search_km = 3\n",
    "\n",
    "# loop through each cast:\n",
    "for jj in range(5,len(FF)):\n",
    "    print('working on ' + str(jj+1)+'/'+str(len(FF)))\n",
    "    \n",
    "    # load pre-downloaded dataframe with time and lat/lon\n",
    "    e = pd.read_pickle(FF[jj])\n",
    "\n",
    "    # search each cast entry: \n",
    "    for kk in range(len(e)):\n",
    "        # define a search region around the cast \n",
    "        lat = float(e.loc[kk,'latitude'])\n",
    "        lon = float(e.loc[kk,'longitude'])\n",
    "        poly = buoy_bound_box(lat,lon,search_km)\n",
    "    \n",
    "        # add a time buffer to search for relevant sat data for each cast\n",
    "        t = datetime.fromisoformat(e.loc[kk,'time'])\n",
    "        t_start = (t-timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        t_end = (t+timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Build ATL03 Request\n",
    "        parms = {\"poly\": poly,\n",
    "                 \"t0\": t_start,\n",
    "                 \"t1\": t_end,\n",
    "                 \"track\": 0,\n",
    "                 \"len\": 20.0,\n",
    "                 \"pass_invalid\": True,\n",
    "                 \"cnf\": -2, # returns all photons\n",
    "                 \"srt\": icesat2.SRT_OCEAN}\n",
    "        atl_gdb = icesat2.atl03sp(parms)\n",
    "    \n",
    "       \n",
    "        if len(atl_gdb)>0:            \n",
    "            # atl_gdb.to_pickle('icesat2_'+str(e.dataset_id)+'.pkl')\n",
    "            print('success '+ FF[jj][:-4] + ' ' + t_start)\n",
    "            print('no. of photons: '+str(len(atl_gdb)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
