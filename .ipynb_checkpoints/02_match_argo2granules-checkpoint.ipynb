{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeebceb-4eeb-4e6c-90f0-be6360b21969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The goal is to find matching ICESat-2 ATL03 photon data matching the times and lat/lon of the published Kd values from BGC ARGO floats (10.5281/zenodo.8228242). \n",
    "\n",
    "The Kd CSV file was edited after download from Zenodo. Kd calculations were sorted by date. This pre-processing could jsut as easily be done in python but I already had the csv open to look at the contents. Edited version is called Dataset_Kd_Paper_2018. \n",
    "\n",
    "Actions:\n",
    "- Load lat/lon and time for each Kd calculation\n",
    "- Check if there is a matching pass for ICESat-2 within +- x hours and  x km.\n",
    "- The notebook saves pickle files of GeoDataFrames (pandas), appending the matching row of the spreadsheet of latlontimes. \n",
    "- These GDFs are the output of the icesat2.atl03sp search function. The contents of each GDF can be found here: https://slideruleearth.io/web/rtd/user_guide/ICESat-2.html#photon-segments\n",
    "- Also saves a new copy of Dataset_Kd_Paper_2018_dep with only the matching kd rows.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f821b93-0ec9-41db-af0e-61e26934504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sliderule import sliderule, icesat2, earthdata\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "sliderule.init(verbose=False)\n",
    "\n",
    "\n",
    "def buoy_bound_box(lat,lon,buffer_km):\n",
    "    # define a buffer distance around the buoy to search for icesat-2 data\n",
    "    lat_buff = buffer_km/111 # convert buffer distance to frac of 1 deg lat\n",
    "    lon_buff = buffer_km/(111*np.cos(lat*np.pi/180)) # convert buffer distance to frac of 1 deg lon\n",
    "    # define bounding box around the buoy (WSEN)\n",
    "    # example: bbox = [-108.3, 39.2, -107.8, 38.8]\n",
    "    # bbox = [lon-lon_buff,lat+lat_buff,lon+lon_buff,lat-lat_buff]\n",
    "    # region = sliderule.toregion(bbox)\n",
    "    minx = lon - lon_buff\n",
    "    miny = lat - lat_buff\n",
    "    maxx = lon + lon_buff\n",
    "    maxy = lat + lat_buff\n",
    "\n",
    "    poly = [{'lon': minx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': miny}] # Closing the loop by repeating the first point\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d72dbef-f954-499f-be76-c1de3233b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time, lat and lon\n",
    "df = pd.read_csv(\"Dataset_Kd_Paper_2018.csv\")\n",
    "# convert matlab time to datetime objects\n",
    "df[\"dt_float\"] = pd.to_datetime(df[\"dt_float\"]-719529,unit='d',utc=True).round('s')\n",
    "# remove all rows from before icesat2 launched\n",
    "df = df[df[\"dt_float\"]>datetime.fromisoformat('2018-10-01T00:00:00Z')]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cbc4d3-802a-4e98-9220-8b847742bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1200/5129\n",
      "processing 1300/5129\n",
      "processing 1400/5129\n",
      "processing 1500/5129\n",
      "processing 1600/5129\n",
      "processing 1700/5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception <-1>: Failure on resource ATL03_20191107102440_06340507_006_01.h5 track 1.0: H5Coro::Future read failure on /gt1l/geolocation/reference_photon_lat\n",
      "Exception <-1>: Failure on resource ATL03_20191107102440_06340507_006_01.h5 track 1.0: H5Coro::Future read failure on /gt1l/geolocation/reference_photon_lat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1800/5129\n",
      "processing 1900/5129\n",
      "processing 2000/5129\n",
      "processing 2100/5129\n",
      "processing 2200/5129\n",
      "processing 2300/5129\n",
      "processing 2400/5129\n",
      "processing 2500/5129\n",
      "processing 2600/5129\n",
      "processing 2700/5129\n",
      "processing 2800/5129\n",
      "processing 2900/5129\n",
      "processing 3000/5129\n",
      "processing 3100/5129\n",
      "processing 3200/5129\n",
      "no. of photons: 32074\n",
      "no. of photons: 7391\n",
      "processing 3300/5129\n",
      "processing 3400/5129\n",
      "processing 3500/5129\n",
      "processing 3600/5129\n",
      "processing 3700/5129\n",
      "processing 3800/5129\n",
      "no. of photons: 29344\n",
      "no. of photons: 29344\n",
      "processing 3900/5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection error to endpoint https://sliderule.slideruleearth.io/source/atl03sp ...retrying request\n",
      "Connection error to endpoint https://sliderule.slideruleearth.io/source/atl03sp ...retrying request\n",
      "Connection error to endpoint https://sliderule.slideruleearth.io/source/atl03sp ...retrying request\n",
      "Unable to complete request due to errors\n",
      "Connection error to endpoint https://sliderule.slideruleearth.io/source/atl03sp ...retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of photons: 303380\n",
      "processing 4000/5129\n",
      "processing 4100/5129\n",
      "no. of photons: 54464\n",
      "no. of photons: 54464\n",
      "no. of photons: 54464\n",
      "no. of photons: 54464\n",
      "processing 4200/5129\n",
      "processing 4300/5129\n",
      "processing 4400/5129\n",
      "processing 4500/5129\n",
      "processing 4600/5129\n",
      "processing 4700/5129\n",
      "processing 4800/5129\n",
      "processing 4900/5129\n",
      "processing 5000/5129\n",
      "no. of photons: 46967\n",
      "no. of photons: 46967\n",
      "no. of photons: 71026\n",
      "no. of photons: 71026\n",
      "processing 5100/5129\n"
     ]
    }
   ],
   "source": [
    "df[\"check_sum\"] = False\n",
    "# these values can be adjusted to broaden/narrow the fit btwn icesat-2 and the ground truth\n",
    "search_hrs = 3\n",
    "search_km = 3\n",
    "for jj in range(len(df)):\n",
    "    if jj % 100 ==0:\n",
    "        print('processing '+str(jj) +'/'+str(len(df)))# give a printout every 100 for my sanity\n",
    "    # define a search region around the buoy \n",
    "    lat = df['lat_float'][jj]\n",
    "    lon = df['lon_float'][jj]\n",
    "    \n",
    "    poly = buoy_bound_box(lat,lon,search_km)\n",
    "\n",
    "    t_start = (df['dt_float'][jj]-timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    t_end = (df['dt_float'][jj]+timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    parms = {\"poly\": poly,\n",
    "             \"t0\": t_start,\n",
    "             \"t1\": t_end,\n",
    "             \"track\": 0,\n",
    "             \"pass_invalid\": True,\n",
    "             \"cnf\": -2, # returns all photons\n",
    "             \"srt\": icesat2.SRT_OCEAN\n",
    "            }\n",
    "\n",
    "    atl_gdb = icesat2.atl03sp(parms)\n",
    "    if len(atl_gdb)>0:\n",
    "        df.loc[jj,\"check_sum\"] = True\n",
    "        print('no. of photons: '+str(len(atl_gdb)))\n",
    "        atl_gdb.to_pickle('icesat2_'+str(jj)+'.pkl')\n",
    "    del atl_gdb\n",
    "    time.sleep(1) #avoid overloading the cmr server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e91ffef-8a80-4cbe-ac5b-3fd2c41e4842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_pickle('glider_matches.pkl')\n",
    "df=df[df[\"check_sum\"]==True]\n",
    "print(len(df))\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv('Dataset_Kd_Paper_2018_dep_3km3h.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
