{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842adedb",
   "metadata": {},
   "source": [
    "Goal: find all stations with any relevant data in Oct 2018 - Dec 2023\n",
    "\n",
    "Order of operations:\n",
    "- Find all active stations in this time frame\n",
    "- get dataset info for each that could match \n",
    "- check variable names for key words\n",
    "- save that dataset location and active time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8e3110-5c8f-47e5-b807-23e3bedb6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --quiet erddapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141611e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from erddapy import ERDDAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import json \n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# help(ERDDAP.get_search_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6dbb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62,)\n"
     ]
    }
   ],
   "source": [
    "# load a list of all the ERDDAPs and their urls.\n",
    "# This is saved as a JSON with the code bc I cant get the direct link to the git site to work.\n",
    "f = open('erddaps.json')\n",
    "data_json = pd.read_json(f)\n",
    "erddap_links = data_json['url'].unique()\n",
    "\n",
    "erddap_links=erddap_links\n",
    "\n",
    "print(erddap_links)\n",
    "# # load in all possible erddap servers straight from site (not working...why??):\n",
    "# url = 'https://github.com/IrishMarineInstitute/awesome-erddap/blob/df70c66a0784a384f398492d356df46dae4281d4/erddaps.json'\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data_json = json.loads(response.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606c95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "----No matching sites exist\n",
      "----No matching sites exist\n",
      "-Saving matching sites\n",
      "32426\n"
     ]
    }
   ],
   "source": [
    "# try searching for all possibly matching stations for this time (there will be a ton)\n",
    "\n",
    "\n",
    "min_time = \"2018-10-01T00:00:00Z\"\n",
    "max_time = \"2024-01-01T00:00:00Z\"\n",
    "\n",
    "kw = {\n",
    "    \"min_time\": min_time,\n",
    "    \"max_time\": max_time,\n",
    "}\n",
    "\n",
    "# save a dataframe of every station that has data in the range\n",
    "# this is a fast loop that makes the next (very slow) loop faster\n",
    "\n",
    "# initialize empty lists\n",
    "allsites = list()\n",
    "allurl = []\n",
    "for orgs in erddap_links:\n",
    "    # print('Searching ' + orgs)\n",
    "    \n",
    "    # start making the erddap object for this erddap url \n",
    "    e = ERDDAP(\n",
    "        server=orgs, \n",
    "        protocol=\"tabledap\", # Want table data (not a grid map of data) \n",
    "        response=\"csv\") #in csv format for pandas\n",
    "\n",
    "    url = e.get_search_url(search_for='', response=\"csv\", **kw)# search for a match to our time/location\n",
    "    try:\n",
    "        temp = pd.read_csv(url)[\"Dataset ID\"].unique() # try to save the data\n",
    "    except:\n",
    "        print(\"----No matching sites exist\") # ignore this entry if there is no data\n",
    "    else:\n",
    "        print(\"-Saving matching sites\")\n",
    "        allsites.extend(temp) # save the site name(s)\n",
    "        for _ in range(len(temp)): allurl.append(orgs) # save a url for each site\n",
    "\n",
    "# add blank columns for saving the time range and lat/lon range for each match\n",
    "FF = pd.DataFrame(allsites,columns=['sites'])\n",
    "FF = FF.assign(url=allurl)\n",
    "FF[\"time_coverage_start\"] = np.nan\n",
    "FF[\"time_coverage_end\"] = np.nan\n",
    "FF[\"geospatial_lat_min\"] = np.nan\n",
    "FF[\"geospatial_lat_max\"] = np.nan\n",
    "FF[\"geospatial_lon_min\"] = np.nan\n",
    "FF[\"geospatial_lon_max\"] = np.nan\n",
    "FF[\"check_sum\"] = 1\n",
    "# note: some sites have messy metadata (eg listing lat/lon everywhere) and will show up as a match even when there is no relevant data.\n",
    "print(len(FF))\n",
    "del e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f14c6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n"
     ]
    }
   ],
   "source": [
    "# now search for matching keywords in each erddap site found above (~40k entries)\n",
    "# I only pull in metadata from ERDDAP (no actual data)\n",
    "# this cell takes a looooong time (2 h?)\n",
    "searchterms = r\"chl|turb|sedi|par|phot|secchi|ssc\"\n",
    "\n",
    "for jj in range(len(FF)):\n",
    "    if jj % 100 ==0:\n",
    "        print(jj)# give a printout every 100 for my sanity\n",
    "    \n",
    "    # make the info URL for this site for the full icesat-2 time range\n",
    "    e = ERDDAP(server=FF['url'][jj], protocol=\"tabledap\", response=\"csv\")\n",
    "    e.dataset_id = FF['sites'][jj]\n",
    "    e.constraints = {\"time>=\": min_time, \"time<=\": max_time}\n",
    "    info_url = e.get_info_url()\n",
    "#     print(info_url)\n",
    "\n",
    "    # make a dataframe for all the metadata for this station\n",
    "    # some of these urls are bogus - if fail then fill with NaNs\n",
    "    try:\n",
    "        df = pd.read_csv(info_url)# make a dataframe for all the metadata for this station\n",
    "    except:\n",
    "        FF.loc[jj,\"check_sum\"] = np.nan\n",
    "#         print(str(jj),' does not exist')    \n",
    "    else:\n",
    "        # search the Variable Names for relevant measurements and minimum amount of metadata (lat/lon)\n",
    "        idx = df[\"Variable Name\"].str.contains(searchterms, flags = re.IGNORECASE)\n",
    "        if any(idx==True)==True:# if there IS a relevant variable        \n",
    "            e.constraints = {}# find the entire time range for the station and its location\n",
    "            info_url = e.get_info_url() # just get metadata instead of downloading data(e.get_download_url)\n",
    "            try:\n",
    "                df = pd.read_csv(info_url)# make a dataframe for all the metadata for this station\n",
    "            except:\n",
    "                FF.loc[jj,\"check_sum\"] = np.nan\n",
    "#                 print(str(jj),' does not exist')\n",
    "            else:\n",
    "\n",
    "                # sometimes metadata is missing - leave blank if any is empty\n",
    "                try:\n",
    "                    FF.loc[jj,\"time_coverage_start\"] = df.loc[df['Attribute Name']=='time_coverage_start', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    FF.loc[jj,\"time_coverage_end\"] = df.loc[df['Attribute Name']=='time_coverage_end', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    FF.loc[jj,\"geospatial_lat_min\"] = df.loc[df['Attribute Name']=='geospatial_lat_min', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    FF.loc[jj,\"geospatial_lat_max\"] = df.loc[df['Attribute Name']=='geospatial_lat_max', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    FF.loc[jj,\"geospatial_lon_min\"] = df.loc[df['Attribute Name']=='geospatial_lon_min', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    FF.loc[jj,\"geospatial_lon_max\"] = df.loc[df['Attribute Name']=='geospatial_lon_max', 'Value'].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        else: # otherwise ignore this site\n",
    "#             print(str(jj),' has no relevant data')\n",
    "            FF.loc[jj,\"check_sum\"] = np.nan\n",
    "\n",
    "# remove any entries that failed\n",
    "FF = FF.dropna(subset=\"check_sum\")  \n",
    "FF.reset_index(drop=True, inplace=True)\n",
    "FF = FF.drop(columns=\"check_sum\")\n",
    "# make any text entries into actual numbers\n",
    "FF[\"geospatial_lat_min\"] = pd.to_numeric(FF[\"geospatial_lat_min\"])\n",
    "FF[\"geospatial_lon_min\"] = pd.to_numeric(FF[\"geospatial_lon_min\"])\n",
    "FF[\"geospatial_lat_max\"] = pd.to_numeric(FF[\"geospatial_lat_max\"])\n",
    "FF[\"geospatial_lon_max\"] = pd.to_numeric(FF[\"geospatial_lon_max\"])\n",
    "# save for future reference\n",
    "FF.to_pickle(\"all_relevant_stations.pkl\")\n",
    "\n",
    "# also save a csv for curious viewers\n",
    "FF_save.to_csv('erddap_station_search_results.csv',index=False)\n",
    "print(len(FF_save))\n",
    "FF_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running that slow code I realized that I didnt save enough information...\n",
    "\n",
    "\n",
    "# add columns to identify which type of data is present in each dataset\n",
    "lib = {'buoy','glider','ship','turbid','ssc','photo','chl','radiation'}\n",
    "\n",
    "FF[\"buoy\"] = False\n",
    "FF[\"glider\"] = False\n",
    "FF[\"ship\"] = False\n",
    "FF[\"turbid\"] = False\n",
    "FF[\"ssc\"] = False\n",
    "FF[\"photo\"] = False\n",
    "FF[\"chl\"] = False\n",
    "FF['radiation'] = False\n",
    "FF['bad'] = False\n",
    "\n",
    "for jj in range(len(FF)):\n",
    "    if jj % 100 ==0:\n",
    "        print(jj)# give a printout every 100 for my sanity\n",
    "\n",
    "    # make the info URL for this site for this time range\n",
    "    e = ERDDAP(server=FF['url'][jj],\n",
    "               protocol=\"tabledap\",\n",
    "               response=\"csv\"\n",
    "              )\n",
    "    e.dataset_id = FF['sites'][jj]\n",
    "    # e.constraints = {\"time>=\": min_time, \"time<=\": max_time}\n",
    "    info_url = e.get_info_url()\n",
    "    try:\n",
    "        ds_info = pd.read_csv(info_url)# make a dataframe for all the metadata for this station\n",
    "    except:\n",
    "        FF.loc[jj,\"bad\"] = True\n",
    "    else:\n",
    "        for kwd in lib:\n",
    "            idx = ds_info[\"Value\"].str.contains(kwd, flags = re.IGNORECASE)\n",
    "            if any(idx==True)==True:\n",
    "                FF.loc[jj,kwd] = True\n",
    "\n",
    "FF = FF[FF.bad == False] \n",
    "FF.reset_index(drop=True, inplace=True)\n",
    "FF = FF.drop(columns=\"bad\")\n",
    "FF.to_pickle(\"labeled_relevant_stations.pkl\")\n",
    "\n",
    "# now we can look at how many stations are available for each data type\n",
    "print(len(FF[(FF.buoy == True) & (FF.radiation == True)]))\n",
    "FF[(FF.ship == True) & (FF.radiation == True)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
