{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ee7b13-ffc0-4421-97cb-d0521eda0e26",
   "metadata": {},
   "source": [
    "Goal: pull out 1 buoy record that has ssc&chla and search for matching icesat2 granules\n",
    "\n",
    "To do:\n",
    "- Load info for 1 relevant station\n",
    "- Find all matching atl granules and save times from file names\n",
    "- Compare times to see if any matching buoy times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac0bb6-2b05-466a-b79e-2ca653da7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet erddapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950557e2-e65e-41e4-9e4e-e74111ba592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client (version (4, 5, 3)) is out of date with the server (version (4, 6, 4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from erddapy import ERDDAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sliderule import sliderule, icesat2, earthdata\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sliderule.init(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65db29b3-ac6e-44bb-a73c-65eb4949880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname2datetime(fname):\n",
    "    y = int(fname[6:10])\n",
    "    m = int(fname[10:12])\n",
    "    d = int(fname[12:14])\n",
    "    H = int(fname[14:16])\n",
    "    M = int(fname[16:18])\n",
    "    S = int(fname[18:20])\n",
    "\n",
    "    t = datetime(y,m,d,H,M,S, tzinfo=timezone.utc)\n",
    "    return t\n",
    "\n",
    "def buoy_bound_box(lat,lon,buffer_km):\n",
    "    # define a buffer distance around the buoy to search for icesat-2 data\n",
    "    lat_buff = buffer_km/111 # convert buffer distance to frac of 1 deg lat\n",
    "    lon_buff = buffer_km/(111*np.cos(lat*np.pi/180)) # convert buffer distance to frac of 1 deg lon\n",
    "    # define bounding box around the buoy (WSEN)\n",
    "    # example: bbox = [-108.3, 39.2, -107.8, 38.8]\n",
    "    # bbox = [lon-lon_buff,lat+lat_buff,lon+lon_buff,lat-lat_buff]\n",
    "    # region = sliderule.toregion(bbox)\n",
    "    minx = lon - lon_buff\n",
    "    miny = lat - lat_buff\n",
    "    maxx = lon + lon_buff\n",
    "    maxy = lat + lat_buff\n",
    "\n",
    "    poly = [{'lon': minx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': miny},\n",
    "            {'lon': maxx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': maxy},\n",
    "            {'lon': minx, 'lat': miny}] # Closing the loop by repeating the first point\n",
    "    return poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132b8ab8-f0fb-44d2-ba8e-14c52ca4362f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a list of all the relevant ERDDAPs and their urls\n",
    "FF = pd.read_pickle(\"labeled_relevant_stations.pkl\")\n",
    "FF = FF[(FF.buoy == True) & (FF.radiation==True)]\n",
    "FF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "FF[\"geospatial_lat_min\"] = pd.to_numeric(FF[\"geospatial_lat_min\"])\n",
    "FF[\"geospatial_lon_min\"] = pd.to_numeric(FF[\"geospatial_lon_min\"])\n",
    "FF[\"geospatial_lat_max\"] = pd.to_numeric(FF[\"geospatial_lat_max\"])\n",
    "FF[\"geospatial_lon_max\"] = pd.to_numeric(FF[\"geospatial_lon_max\"])\n",
    "FF[\"photon_data\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5d7a0-4f0a-412e-ae47-70428e04ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 3/42\n",
      "working on 4/42\n",
      "success edu_humboldt_humboldt 2019-01-05T18:41:42Z\n",
      "no. of photons: 620933\n"
     ]
    }
   ],
   "source": [
    "# search parameters\n",
    "search_hrs = 1\n",
    "search_km = 2\n",
    "# loop through each buoy asset:\n",
    "# for jj in range(len(FF)):\n",
    "for jj in range(2,5):\n",
    "    print('working on ' + str(jj+1)+'/'+str(len(FF)))\n",
    "    # define a search region around the buoy \n",
    "    lat = FF['geospatial_lat_min'][jj]\n",
    "    lon = FF['geospatial_lon_min'][jj]\n",
    "    poly = buoy_bound_box(lat,lon,search_km)\n",
    "\n",
    "    # search CMR for ATL03 granules in the bounding box\n",
    "    grns = earthdata.cmr(short_name=\"ATL03\",\n",
    "                         polygon=poly,\n",
    "                         version='006')\n",
    "    # save the times for each granule as a datetime object\n",
    "    icesat_times = [fname2datetime(fname) for fname in grns]\n",
    "\n",
    "    # set up erddap search for this server:\n",
    "    e = ERDDAP(\n",
    "    server=FF['sites'][jj], \n",
    "    protocol=\"tabledap\", # Want table data (not a grid map of data) \n",
    "    response=\"csv\") #in csv format for pandas\n",
    "\n",
    "    # now check if buoy data exists for these granules\\n\",\n",
    "    e = ERDDAP(server=FF['url'][jj],\n",
    "               protocol=\"tabledap\",\n",
    "               response=\"csv\")\n",
    "    e.dataset_id = FF['sites'][jj]\n",
    "\n",
    "    for t in icesat_times:\n",
    "        # add a time buffer (+/-1 hours) to search for relevant buoy data for each granule,\n",
    "        t_start = (t-timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        t_end = (t+timedelta(hours=search_hrs)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        e.constraints = {\"time>=\": t_start,\n",
    "                        \"time<=\": t_end}\n",
    "\n",
    "        # try to download the associated buoy data\n",
    "        try:\n",
    "            buoy = e.to_pandas(parse_dates=True)           \n",
    "        except:\n",
    "            continue\n",
    "        # if buoy data exists, download the ATL03 photons in the bounding box at this time\n",
    "        \n",
    "        # Build ATL03 Request\n",
    "        poly = buoy_bound_box(lat,lon,search_km)       \n",
    "        parms = {\"poly\": poly,\n",
    "                 \"t0\": t_start,\n",
    "                 \"t1\": t_end,\n",
    "                 \"len\": 100.0,\n",
    "                 \"track\": 0,\n",
    "                 \"pass_invalid\": True,\n",
    "                 \"cnf\": -2, # returns all photons\n",
    "                 \"srt\": icesat2.SRT_OCEAN}\n",
    "        atl_gdb = icesat2.atl03sp(parms)\n",
    "        \n",
    "        if len(atl_gdb)>0:            \n",
    "            FF.loc[jj,\"photon_data\"] = True            \n",
    "            atl_gdb.to_pickle('icesat2_'+str(e.dataset_id)+'_'+str(t_start)+'.pkl')\n",
    "            print('success '+ e.dataset_id + ' ' + str(t_start))\n",
    "            print('no. of photons: '+str(len(atl_gdb)))\n",
    "            # buoy.to_csv('data_'+str(e.dataset_id)+'_'+str(t_start)+'.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
